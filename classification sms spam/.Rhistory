x = data.frame(1,2,3,4)
x
colnames(x1) = "text"
colnames(x) = "text"
x
colnames(x) = "text",'a'
colnames(x) = ['a','b','c','d']
colnames(x) <- c('a','b','c','d')
x
install.packages("RoughSets")
data(RoughSetData)
include(RoughSets)
require(RoughSets)
data(RoughSetData)
data(RoughSets)
require(RoughSets)
data(RoughSets)
decision.table <- RoughSetData$hiring.dt
## define considered attributes which are first, second, and
## third attributes
attr.P <- c(1,2,3)
## compute indiscernibility relation
IND <- BC.IND.relation.RST(decision.table, attribute = attr.P)
## compute lower and upper approximations
## Let us define fourth index as the decision attribute
decision.attr <- c(4)
roughset <- BC.LU.approximation.RST(decision.table, IND, decision.attr)
## Determine regions
region.RST <- BC.positive.reg.RST(decision.table, roughset)
## The decision-relative discernibility matrix and reduct
disc.mat <- BC.discernibility.mat.RST(decision.table, range.object = NULL)
require(RoughSets)
data(RoughSetData)
decision.table <- RoughSetData$hiring.dt
## define considered attributes which are first, second, and
## third attributes
attr.P <- c(1,2,3)
## compute indiscernibility relation
IND <- BC.IND.relation.RST(decision.table, attribute = attr.P)
## compute lower and upper approximations
## Let us define fourth index as the decision attribute
decision.attr <- c(4)
roughset <- BC.LU.approximation.RST(decision.table, IND, decision.attr)
## Determine regions
region.RST <- BC.positive.reg.RST(decision.table, roughset)
## The decision-relative discernibility matrix and reduct
disc.mat <- BC.discernibility.mat.RST(decision.table, range.object = NULL)
require(RoughSets)
data(RoughSetData)
decision.table <- RoughSetData$hiring.dt
View(decision.table)
View(decision.table)
## define considered attributes which are first, second, and
## third attributes
attr.P <- c(1,2,3)
## compute indiscernibility relation
IND <- BC.IND.relation.RST(decision.table, attribute = attr.P)
## compute lower and upper approximations
## Let us define fourth index as the decision attribute
decision.attr <- c(4)
roughset <- BC.LU.approximation.RST(decision.table, IND, decision.attr)
## Determine regions
region.RST <- BC.positive.reg.RST(decision.table, roughset)
## The decision-relative discernibility matrix and reduct
disc.mat <- BC.discernibility.mat.RST(decision.table, range.object = NULL)
5*6
library(tm)
tm
inspect
inspect(ovid[1:2])
install.packages("twitteR")
library(twitteR)
public_tweets = publicTimeline()
library(twitteR)
public_tweets = publicTimeline()
setup_twitter_auth
setup_twitter_auth()
setup_twitter_oauth()
consumer_key = SJ2Rr5c2FFgQnqOk0tieDDHBs
consumer_key = 'SJ2Rr5c2FFgQnqOk0tieDDHBs'
consumer_secret = 'rVP9clKJBBJjSrlDnb7modQy6k1DMkTVwWrIZORFMDgOWkgzed'
setup_twitter_oauth(consumer_key, consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret)
access_token = '305398266-YmJVwEbcIbZS6Kj5PNKhr954VDmyYMZCpxEtpoNG'
access_secret = 'eRxJrOx4vOn06bobXOLniDHyiM28Ja1hQUtsRPBjGTpxx'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
woeid_aus = '23424748'
getTrends(woeid_aus)
searchTwitter('world cup')
searchTwitter('John Deere')
searchTwitter('kubota')
searchTwitter('kubota tractors')
install.packages('RCurl')
#library
library(RCurl)
# Set SSL certs globally
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
library(twitteR)
access_token = '305398266-YmJVwEbcIbZS6Kj5PNKhr954VDmyYMZCpxEtpoNG'consumer_key = 'SJ2Rr5c2FFgQnqOk0tieDDHBs'
#library
library(RCurl)
# Set SSL certs globally
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
library(twitteR)
# Data for access tokens
access_token = '305398266-YmJVwEbcIbZS6Kj5PNKhr954VDmyYMZCpxEtpoNG'
access_secret = 'eRxJrOx4vOn06bobXOLniDHyiM28Ja1hQUtsRPBjGTpxx'
consumer_key = 'SJ2Rr5c2FFgQnqOk0tieDDHBs'
consumer_secret = 'rVP9clKJBBJjSrlDnb7modQy6k1DMkTVwWrIZORFMDgOWkgzed'
# Setup twitter oAuth with token data
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
install.packages("wordcloud")
install.packages("tm")
install.packages('RColorBrewer')
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
mh370 <- searchTwitter("#PrayForMH370", since = "2014-03-08", until = "2014-03-20", n = 1000)
mh370_text = sapply(mh370, function(x) x$getText())
mh370_corpus = Corpus(VectorSource(mh370_text))
tdm = TermDocumentMatrix(
mh370_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("prayformh370", "prayformh", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
mh370 <- searchTwitter("#JohnDeere", since = "2014-03-08", until = "2014-03-20", n = 1000)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
mh370 <- searchTwitter("#JohnDeere", n = 1000)
jd <- searchTwitter("#JohnDeere", n = 1000)
jd_text = sapply(jd, function(x) x$getText())
jd_text[1:10]
jd_corpus = Corpus(VectorSource(jd_text))
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("prayformh370", "prayformh",stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
jd_corpus
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = TRUE,
removeNumbers = TRUE, tolower = TRUE)
)
install.packages("SnowballC")
library(SnowballC)
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = TRUE,
removeNumbers = TRUE, tolower = TRUE)
)
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
library(SnowballC)
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
tdm = TermDocumentMatrix(
jd_corpus[0:300],
control = list(
removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
jd_text <- iconv(jd_text,to="utf-8-mac")
jd_corpus = Corpus(VectorSource(jd_text))
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c(stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
View(m)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
View(dm)
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("johndeere","deere","john",stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
jd <- searchTwitter("John Deere", n = 1000)
jd_text = sapply(jd, function(x) x$getText())
jd_text <- iconv(jd_text,to="utf-8-mac")  #encoding for MAC
jd_corpus = Corpus(VectorSource(jd_text))
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("johndeere","deere","john",stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("read","full","tractor",johndeere","deere","john",stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("read","full","tractor",johndeere","deere","john",stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("read","full","tractor","johndeere","deere","john",stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
save.image("~/Documents/R workspace/work.RData")
jd <- searchTwitter("John Deere", n = 1000, since='2014-01-01')
jd_text = sapply(jd, function(x) x$getText())
jd_text <- iconv(jd_text,to="utf-8-mac")  #encoding for MAC
jd_corpus = Corpus(VectorSource(jd_text))
tdm = TermDocumentMatrix(
jd_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("read","full","tractor","johndeere","deere","john",stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
install.pacakages("swirl")
install.packages("swirl")
library(swirl)
swirl()
setwd("~/Documents/R workspace")
setwd("~/Documents/R workspace/classification sms spam")
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
str(sms_raw)
sms_raw$type[:10]
sms_raw$type[,10]
sms_raw$type
sms_raw$type[1]
sms_raw$type[2]
sms_raw$type[]
sms_raw$type[1:10]
class(sms_raw$type)
sms_raw$type <- factor(sms_raw$type)
class(sms_raw$type)
sms_raw$type[1:10]
str(sms_raw$type)
table(sms_raw$type)
library(tm)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
sms_corpus[1]
sms_corpus[2]
sms_corpus$text
sms_corpus
head(sms_corpus)
print(sms_corpus)
inspect(sms_corpus[1:3])
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuations)
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
inspect(sms_corpus[1:3])
inspect(corpus_clen[1:3])
inspect(corpus_clean[1:3])
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
# comparison before cleaning and after cleaning
inspect(sms_corpus[1:3])
inspect(corpus_clean[1:3])
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm <- DocumentTermMatrix(corpus_clean)
inspect(corpus_clean[1:3])
sms_dtm <- DocumentTermMatrix(corpus_clean)
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
# comparison before cleaning and after cleaning
inspect(sms_corpus[1:3])
inspect(corpus_clean[1:3])
# Tokenization - converting a sentence into tokens(words)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw_train <- sms_raw[1:4169,]
sms_raw_test <- sms_raw[4170:5559,]
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5559,]
sms_corpus_train <- corpus_clean[1:4169,]
sms_corpus_test <- corpus_clean[4170:5559,]
sms_corpus_train <- corpus_clean[1:4169,]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5559]
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
spam <- subset(sms_raw_train, type == "spam")
ham <- subset(sms_raw_train, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
